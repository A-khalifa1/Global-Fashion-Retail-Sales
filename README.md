# ğŸ›ï¸ Global Fashion Retail Sales - Lambda-Inspired Architecture Project

This project demonstrates a real-world **Lambda-Inspired Data Architecture** for analyzing fashion retail sales across multiple countries, using **separate dashboards** for batch and streaming layers.

It simulates a hybrid setup combining historical data in a data warehouse with real-time transaction data streamed from a cashier system.

---

## ğŸŒ Project Overview

A fashion retail company operates stores across 7 countries (Brazil, USA, China, France, Germany, etc.).  
The goal is to build a dual-pipeline system that:

- Handles **historical data** for deep analytics.
- Handles **live transaction data** for real-time monitoring.
- Presents both views via **dedicated dashboards**.


  ![diagram](diagram.png)

---

## ğŸ§± Architecture Summary

This solution adopts a **Lambda-Inspired Architecture**, split into two separate paths:

### 1ï¸âƒ£ Batch Layer (Historical Data)
- **Source:**  
  Historical data is stored in **Microsoft SQL Server**.

- **ETL Process:**  
  Data is extracted and transformed using **SSIS**, then loaded into a **Data Warehouse** (also SQL Server).

- **Visualization:**  
  The data warehouse is connected to **Power BI**, which provides a rich analytical dashboard.

---

### 2ï¸âƒ£ Streaming Layer (Live Data Simulation)
- **Simulation:**  
  Live cashier transactions are **simulated using Python** and streamed in real-time.

- **Streaming Pipeline:**  
  - Data is sent to **Apache Kafka** (via Kafka Mini on Docker).
  - A **Kafka Consumer** written in Python receives the data.
  - The consumer writes the streamed records to a **CSV file**.

- **Visualization:**  
  A **Streamlit app** reads from the CSV and presents **live KPIs** such as:
  - Latest transactions
  - Total quantity sold
  - Real-time revenue

---

## ğŸ” Data Flow

```text
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚   CSV Source File  â”‚  (Used for simulation only)
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ Batch Path (DB)    â”‚
           â”‚ - SQL Server       â”‚
           â”‚ - SSIS             â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ Data Warehouse     â”‚
           â”‚ - SQL Server DW    â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Power BI   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€ Historical Dashboard
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  (Parallel Stream)
                    â–²
                    â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ Kafka Producer    â”‚â—„â”€â”€â”€â”€ Simulated by Python
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Kafka      â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ Kafka Consumer    â”‚
           â”‚ - Writes to CSV   â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Streamlit  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€ Real-time Dashboard
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‚ Project Structure

ğŸ“ batch/
   â”œâ”€â”€ SQL Scripts
   â”œâ”€â”€ SSIS Packages
   â””â”€â”€ Power BI Reports (.pbix)

ğŸ“ streaming/
   â”œâ”€â”€ producer.py
   â”œâ”€â”€ consumer.py
   â””â”€â”€ streamlit_app.py

ğŸ“ kafka/
   â””â”€â”€ docker-compose.yml

ğŸ“ data/
   â”œâ”€â”€ sample_source.csv
   â””â”€â”€ live_transactions.csv (stream output)


## ğŸ› ï¸ Tools & Technologies
Microsoft SQL Server â€“ source & warehouse

SSIS â€“ batch ETL pipeline

Apache Kafka (Docker) â€“ real-time messaging

Python â€“ Kafka producer, consumer, Streamlit app

Power BI â€“ analytical dashboard

Streamlit â€“ real-time dashboard


## ğŸ“Š Dashboards

  Power BI: Country-wise trends, product sales, revenue analysis (based on historical warehouse).

  Streamlit: Real-time view of incoming transactions, KPIs, and live metrics

## ğŸ”€ Design Note
This architecture is Lambda-Inspired, but instead of unifying batch and streaming into one serving layer, the system uses:

   Two separated dashboards for better performance and clarity.

  A clear distinction between historical insights (Power BI) and real-time metrics (Streamlit)

## ğŸš€ How to Run


 #  Start Kafka Mini:

    docker-compose up -d

# Run the Kafka Producer (simulated cashier stream):

    python producer.py

# Start the Kafka Consumer (writes to CSV):

    python consumer.py

# Launch Streamlit dashboard:

    streamlit run streamlit_app.py

 ##  Batch Layer:

   Use SSIS to transfer data from SQL Server to warehouse.

   Open .pbix file in Power BI to view historical analysis.

## ğŸ§  Notes

   Deduplication and error handling are considered in the consumer to prevent inflated numbers.

   This setup is ideal for prototyping real-time pipelines before scaling to full cloud-based solutions
